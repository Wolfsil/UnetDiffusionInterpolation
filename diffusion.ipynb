{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "pathTrain=\"C:\\\\Users\\\\82109\\\\Desktop\\\\train\" #학습할 이미지 \n",
    "pathTest=\"C:\\\\Users\\\\82109\\\\Desktop\\\\test\" #벨리데이션 테스트 이미지\n",
    "pathGeneratorTest=\"C:\\\\Users\\\\82109\\\\Desktop\\\\generatorTest\" #생성 테스트용 이미지\n",
    "pathSave=\"C:\\\\Users\\\\82109\\\\Desktop\\\\checkPoint\" #모델 저장할 위치\n",
    "batchSize=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.new(\"RGBA\", (300, 300),color=(255,0,0,255))\n",
    "canvas = Image.new('RGBA', (400,400), color=(0,255,0,255))\n",
    "mask=Image.new('RGBA', (300,300), color=(0,0,0,127))\n",
    "canvas.paste(image,(0,0),mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=400x400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "#해당 디렉토리의 모든 파일명을 가져옴\n",
    "\n",
    "#경로 반환\n",
    "def GetFilePath(path,end=\".gif\"):\n",
    "  gifFileList=os.listdir(path)\n",
    "  gifPath=[]\n",
    "  for name in gifFileList:\n",
    "    if name.endswith(tuple(end)):\n",
    "      gifPath.append(os.path.join(path,name))\n",
    "  return gifPath\n",
    "\n",
    "\n",
    "#못쓰는 데이터를 걸러줌\n",
    "def PreprocessGif(path,frame=5):\n",
    "  gif=Image.open(path)\n",
    "  size=gif.n_frames\n",
    "  gif.close()\n",
    "  if size<frame:\n",
    "    print(path,\": \",size,\"사용불가능\")\n",
    "    os.remove(path=path)\n",
    "  else:\n",
    "    print(path,\": \",size,\" 사용가능\")\n",
    "\n",
    "#gif를 읽고 넘파이 배열로 노멀라이즈해줌\n",
    "def LoadGif(path, paddingSize=32):\n",
    "  gif=Image.open(path)\n",
    "  flip=np.random.randint(0,7)\n",
    "  print(\"이미지 총프레임 \",gif.n_frames)\n",
    "  extractFrame=np.random.randint(2,11)\n",
    "  remainFrame=gif.n_frames-extractFrame\n",
    "  start=0\n",
    "  end=0\n",
    "\n",
    "  if remainFrame<=1:\n",
    "    start=1\n",
    "    end=gif.n_frames\n",
    "  else:\n",
    "    start=np.random.randint(1,remainFrame+1)\n",
    "    end=start+extractFrame\n",
    "  print(\"추출 설정(추출프레임, 가용프레임, 시작, 끝) \",extractFrame,remainFrame,start,end)\n",
    "  images=[]\n",
    "  for i in range(start,end):\n",
    "    gif.seek(i)\n",
    "    temp=gif.transpose(flip).convert(\"RGBA\")\n",
    "    temp=np.array(temp)\n",
    "    height=(paddingSize-temp.shape[0]%paddingSize)%paddingSize\n",
    "    width=(paddingSize-temp.shape[1]%paddingSize)%paddingSize\n",
    "    temp=np.pad(temp, pad_width=((0,height),(0,width),(0,0)),mode=\"constant\",constant_values=0)\n",
    "    images.append(temp)\n",
    "  gif.close()\n",
    "\n",
    "  return np.array(images)/255.0\n",
    "\n",
    "#인풋데이터와 아웃풋 데이터를 분리\n",
    "def Divide(arr):\n",
    "  evens=arr[0::2]\n",
    "  odds=arr[1::2]\n",
    "  if evens.shape[0] != odds.shape[0]:\n",
    "    evens=evens[0:-1]\n",
    "  return [evens,odds]\n",
    "\n",
    "def saveGif(path, images):\n",
    "  imgs=[]\n",
    "  for i in images:\n",
    "    img=Image.fromarray((i*255).round().astype(np.int8), mode=\"RGBA\")\n",
    "    imgs.append(img)\n",
    "  imgs[0].save(path, save_all=True, append_images=imgs[1:], disposal = 2,duration=150, loop=0)\n",
    "  \n",
    "#데이터셋 제너레이터 생성\n",
    "def DatasetGenerater(gifPath):\n",
    "  #gif파일을 반환\n",
    "  for i in gifPath:\n",
    "      x1,y= Divide(LoadGif(i))  \n",
    "      x2=np.random.rand(x1.shape[0],x1.shape[1],x1.shape[2],x1.shape[3])\n",
    "      yield (x1,x2,y)\n",
    "\n",
    "\n",
    "#사용예제 \n",
    "\n",
    "# gifPath=GetFilePath(path) \n",
    "# for i in gifPath:\n",
    "#   PreprocessGif(i)\n",
    "\n",
    "# arr=LoadGif(gifPath[3])\n",
    "# input, output=Divide(arr)\n",
    "# print(arr.shape ,input.shape, output.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트레인셋 전처리\n",
      "C:\\Users\\82109\\Desktop\\datasets\\1.gif :  32  사용가능\n",
      "C:\\Users\\82109\\Desktop\\datasets\\2.gif :  31  사용가능\n",
      "C:\\Users\\82109\\Desktop\\datasets\\3.gif :  128  사용가능\n",
      "C:\\Users\\82109\\Desktop\\datasets\\smoke_loop_animation_by_alexredfish_d8uuq6h.gif :  45  사용가능\n",
      "C:\\Users\\82109\\Desktop\\datasets\\tomoe_kickslash_riser_animation_by_hybridmink_d9yq8fu.gif :  15  사용가능\n",
      "C:\\Users\\82109\\Desktop\\datasets\\train_by_kirokaze_d9q4jxs.gif :  29  사용가능\n",
      "C:\\Users\\82109\\Desktop\\datasets\\t_crouch_kicks_by_hybridmink_d952wjw.gif :  13  사용가능\n",
      "테스트셋 전처리\n",
      "C:\\Users\\82109\\Desktop\\datasetsval\\0.gif :  8  사용가능\n",
      "C:\\Users\\82109\\Desktop\\datasetsval\\1.gif :  32  사용가능\n",
      "C:\\Users\\82109\\Desktop\\datasetsval\\2.gif :  31  사용가능\n",
      "C:\\Users\\82109\\Desktop\\datasetsval\\train_by_kirokaze_d9q4jxs.gif :  29  사용가능\n"
     ]
    }
   ],
   "source": [
    "print(\"트레인셋 전처리\")\n",
    "#사용불가능 파일 전처리\n",
    "gifPath=GetFilePath(pathTrain) \n",
    "for i in gifPath:\n",
    "  PreprocessGif(i)\n",
    "gifPath=GetFilePath(pathTrain)\n",
    "\n",
    "print(\"테스트셋 전처리\")\n",
    "gifPathTest=GetFilePath(pathTest) \n",
    "for i in gifPathTest:\n",
    "  PreprocessGif(i)\n",
    "gifPathTest=GetFilePath(pathTest)\n",
    "\n",
    "\n",
    "trainDataset=tf.data.Dataset.from_generator(DatasetGenerater,\n",
    "                               args=[gifPath], output_types=(tf.float32,tf.float32,tf.float32),\n",
    "                               output_shapes = ((None, None,None,4),(None, None,None,4),(None, None,None,4)))\n",
    "#(inputImages, outputImages)\n",
    "trainDataset=trainDataset.shuffle(10).batch(batchSize).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "testDataset=tf.data.Dataset.from_generator(DatasetGenerater,\n",
    "                               args=[gifPathTest], output_types=(tf.float32,tf.float32,tf.float32),\n",
    "                               output_shapes = ((None, None,None,4),(None, None,None,4),(None, None,None,4)))\n",
    "#(inputImages, outputImages)\n",
    "testDataset=testDataset.shuffle(10).batch(batchSize).prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 총프레임  20\n",
      "추출 설정(추출프레임, 가용프레임, 시작, 끝)  3 17 1 4\n"
     ]
    }
   ],
   "source": [
    "x,_,_=DatasetGenerater(GetFilePath(pathGeneratorTest)).__next__()\n",
    "saveGif(os.path.join(pathSave,\"epoch.gif\"),x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unet 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seperableConv(filter, input):\n",
    "    depthwise=tf.keras.layers.Conv3D(input.shape[-1],3,padding=\"same\",groups=input.shape[-1])(input)\n",
    "    pointwise=tf.keras.layers.Conv3D(filter,1,padding=\"same\")(depthwise)\n",
    "    return pointwise\n",
    "    \n",
    "def block(filter,input):\n",
    "    #conv1=tf.keras.layers.Conv3D(filter,3,padding=\"same\")(input)\n",
    "    conv1=seperableConv(filter,input)\n",
    "    layerNorm1=tf.keras.layers.LayerNormalization()(conv1)\n",
    "    swishAct1=tf.keras.layers.Activation(\"swish\")(layerNorm1)\n",
    "\n",
    "    #conv2=tf.keras.layers.Conv3D(filter,3,padding=\"same\")(swishAct1)\n",
    "    conv2=seperableConv(filter,swishAct1)\n",
    "    layerNorm2=tf.keras.layers.LayerNormalization()(conv2)\n",
    "    swishAct2=tf.keras.layers.Activation(\"swish\")(layerNorm2)\n",
    "    \n",
    "    return swishAct2\n",
    "\n",
    "\n",
    "def unetModel(inputShape=(None, None, None, 4)):\n",
    "    \n",
    "    inputImage=tf.keras.Input(shape=inputShape)\n",
    "    noisyImage=tf.keras.Input(shape=inputShape)\n",
    "    input=tf.keras.layers.Concatenate()([inputImage,noisyImage])\n",
    "    \n",
    "    #인코딩\n",
    "    e1=block(32,input)\n",
    "    e1Pooling=tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(e1)\n",
    "    \n",
    "    e2=block(64,e1Pooling)\n",
    "    e2Pooling=tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(e2)\n",
    "    \n",
    "    e3=block(128,e2Pooling)\n",
    "    e3Pooling=tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(e3)\n",
    "    \n",
    "    e4=block(256,e3Pooling)\n",
    "    e4Pooling=tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(e4)\n",
    "    \n",
    "    e5=block(512,e4Pooling)\n",
    "    e5Pooling=tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(e5)\n",
    "    \n",
    "    #중간\n",
    "    bottleNeck=block(1024,e5Pooling)\n",
    "\n",
    "    d5UpSampling=tf.keras.layers.UpSampling3D(size=(1,2,2))(bottleNeck)\n",
    "    d5Transpose=seperableConv(512,d5UpSampling)\n",
    "    d5Concatenate=tf.keras.layers.Concatenate()([d5Transpose,e5])\n",
    "    d5=block(512,d5Concatenate)\n",
    "    \n",
    "    d4UpSampling=tf.keras.layers.UpSampling3D(size=(1,2,2))(d5)\n",
    "    d4Transpose=seperableConv(256,d4UpSampling)\n",
    "    d4Concatenate=tf.keras.layers.Concatenate()([d4Transpose,e4])\n",
    "    d4=block(256,d4Concatenate)\n",
    "    \n",
    "    d3UpSampling=tf.keras.layers.UpSampling3D(size=(1,2,2))(d4)\n",
    "    d3Transpose=seperableConv(128,d3UpSampling)\n",
    "    d3Concatenate=tf.keras.layers.Concatenate()([d3Transpose,e3])\n",
    "    d3=block(128,d3Concatenate)\n",
    "    \n",
    "    d2UpSampling=tf.keras.layers.UpSampling3D(size=(1,2,2))(d3)\n",
    "    d2Transpose=seperableConv(64,d2UpSampling)\n",
    "    d2Concatenate=tf.keras.layers.Concatenate()([d2Transpose,e2])\n",
    "    d2=block(64,d2Concatenate)\n",
    "    \n",
    "    d1UpSampling=tf.keras.layers.UpSampling3D(size=(1,2,2))(d2)\n",
    "    d1Transpose=seperableConv(32,d1UpSampling)\n",
    "    d1Concatenate=tf.keras.layers.Concatenate()([d1Transpose,e1])\n",
    "    d1=block(32,d1Concatenate)\n",
    "    \n",
    "    outputImage=seperableConv(4,d1)\n",
    "    \n",
    "    return tf.keras.Model([inputImage,noisyImage],outputImage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디퓨전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(tf.keras.Model):\n",
    "    def __init__(self,network:tf.keras.Model,batchSize=1):\n",
    "        super().__init__()\n",
    "        self.network= network\n",
    "        self.batchSize=batchSize\n",
    "    \n",
    "    def train_step(self, images):\n",
    "\n",
    "        inputImages, noises, outputImages=images\n",
    "        #학습할 노이즈 생성\n",
    "        \n",
    "        diffusionTime = tf.random.uniform(\n",
    "            shape=(1,1, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        sigRates, noiseRates= self.diffusionSchedule(diffusionTime)\n",
    "        noisyImage = sigRates * outputImages + noiseRates * noises\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            predinputImages=self.network([inputImages,noisyImage],training=True)\n",
    "            loss=self.compiled_loss(noises,predinputImages)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, images):\n",
    "        inputImages, noises, outputImages=images\n",
    "        \n",
    "        diffusionTime = tf.random.uniform(\n",
    "            shape=(1,1, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        sigRates, noiseRates= self.DiffusionSchedule(diffusionTime)\n",
    "        noisyImage = sigRates * outputImages + noiseRates * noises\n",
    " \n",
    "        predinputImages=self.network([inputImages,noisyImage],training=False)\n",
    "        loss=self.compiled_loss(noises,predinputImages)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def DiffusionSchedule(self, diffusionTime):\n",
    "        #각도가 줄어들음 (y축이 노이즈, x축이 시그널)\n",
    "        startAng=tf.acos(0.99) #  라디안 단위(180/파이), 약 0, 시그널 최대\n",
    "        endAng=tf.acos(0.01) # 약 90도, 노이즈 최대\n",
    "        \n",
    "        diffusionAng=startAng+diffusionTime*(endAng-startAng) #DFT가 1에 가까울수록 노이즈(1에서 시작)\n",
    "        sigRate=tf.cos(diffusionAng) # DFT가 1에 가까울수록 0.01\n",
    "        noiseRate=tf.sin(diffusionAng) # DFT가 1에 가까울수록 0.99\n",
    "        return sigRate, noiseRate\n",
    "        \n",
    "    def Generator(self,inputImage:np.ndarray,diffusionStep):\n",
    "        stepSize=1.0 / diffusionStep\n",
    "\n",
    "        for step in range(diffusionStep):\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        noises=np.random.rand(\n",
    "            1,\n",
    "            self.generatorTestImage.shape[0],\n",
    "            self.generatorTestImage.shape[1],\n",
    "            self.generatorTestImage.shape[2],\n",
    "            self.generatorTestImage.shape[3]\n",
    "        )\n",
    "        for step in range(self.diffusionStep):\n",
    "            self.network([self.generatorTestImage,noises])\n",
    "            \n",
    "    \n",
    "    def GeneratorTest(self, epoch=None, logs=None):\n",
    "        if self.generatorTestImage==None:\n",
    "            return\n",
    "            \n",
    "            \n",
    "        \n",
    "#network([np.random.rand(1,4,64,64,4),np.random.rand(1,4,64,64,4)],training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델생성\n",
    "network=unetModel(inputShape=(None,None,None,4))\n",
    "model=DiffusionModel(network)\n",
    "\n",
    "#콜백생성\n",
    "cpCallback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=pathSave, \n",
    "    verbose=1, \n",
    "    save_weights_only=True)\n",
    "esCallback=tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "rlrCallback=tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    min_lr=1e-5)\n",
    "\n",
    "#컴파일\n",
    "model.compile(\n",
    "    tf.keras.optimizers.experimental.AdamW(\n",
    "        learning_rate=1e-2, weight_decay=1e-4\n",
    "    ),\n",
    "    loss=tf.keras.losses.mean_squared_error,\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    trainDataset,\n",
    "    epochs=100,\n",
    "    batch_size=1,\n",
    "    validation_data=testDataset,\n",
    "    validation_batch_size=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.LambdaCallback(on_epoch_end=model.GeneratorTest),\n",
    "        ,cpCallback,\n",
    "        esCallback,\n",
    "        rlrCallback\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스낵코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.evalu\n",
    "\n",
    "diffusionTime = tf.random.uniform(\n",
    "            shape=(1,10, 1024, 1024, 1), minval=10, maxval=20\n",
    ")\n",
    "diffusionTime2 = tf.random.uniform(\n",
    "    shape=(1,10,1024,1024, 4), minval=0.0, maxval=1.0\n",
    ")\n",
    "\n",
    "savePath=\"C:\\Users\\82109\\Desktop\\checkPoint\\test.ckpt\"\n",
    "\n",
    "model=unetModel(inputShape=(None,None,None,4)) model.save_weights(filepath=savePath)\n",
    "\n",
    "model=unetModel(inputShape=(None,None,None,4)) model.load_weights(filepath=savePath)\n",
    "\n",
    "model=unetModel(inputShape=(None,None,None,4)) model.load_weights(filepath=savePath)\n",
    "\n",
    "unetModel().summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "field",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
