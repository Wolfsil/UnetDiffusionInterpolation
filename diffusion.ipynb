{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"C:/Users/82109/Desktop/datasets\"\n",
    "savePath=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#해당 디렉토리의 모든 파일명을 가져옴\n",
    "\n",
    "gifPath=[]\n",
    "#경로 반환\n",
    "def GetFilePath(path,end=\".gif\"):\n",
    "  gifFileList=os.listdir(path)\n",
    "  gifPath=[]\n",
    "  for name in gifFileList:\n",
    "    if name.endswith(tuple(end)):\n",
    "      gifPath.append(os.path.join(path,name))\n",
    "  return gifPath\n",
    "\n",
    "\n",
    "#못쓰는 데이터를 걸러줌\n",
    "def PreprocessGif(path,frame=5):\n",
    "  gif=Image.open(path)\n",
    "  size=gif.n_frames\n",
    "  gif.close()\n",
    "  if size<frame:\n",
    "    print(path,\": \",size,\"사용불가능\")\n",
    "    os.remove(path=path)\n",
    "  else:\n",
    "    print(path,\": \",size,\" 사용가능\")\n",
    "\n",
    "#gif를 읽고 넘파이 배열로 노멀라이즈해줌\n",
    "def LoadGif(path, paddingSize=32):\n",
    "  gif=Image.open(path)\n",
    "  flip=np.random.randint(0,7,1)[0]\n",
    "\n",
    "  images=[]\n",
    "  for i in range(1,gif.n_frames):\n",
    "    gif.seek(i)\n",
    "    temp=gif.transpose(flip).convert(\"RGBA\")\n",
    "    temp=np.array(temp)\n",
    "    height=(paddingSize-temp.shape[0]%paddingSize)%paddingSize\n",
    "    width=(paddingSize-temp.shape[1]%paddingSize)%paddingSize\n",
    "    temp=np.pad(temp, pad_width=((0,height),(0,width),(0,0)),mode=\"constant\",constant_values=0)\n",
    "    images.append(temp)\n",
    "  gif.close()\n",
    "  return np.array(images)/255.0\n",
    "\n",
    "#인풋데이터와 아웃풋 데이터를 분리\n",
    "def Divide(arr):\n",
    "  evens=arr[0::2]\n",
    "  odds=arr[1::2]\n",
    "  if evens.shape[0] != odds.shape[0]:\n",
    "    evens=evens[0:-1]\n",
    "  return [evens,odds]\n",
    "\n",
    "\n",
    "#사용예제 \n",
    "\n",
    "#gifPath=GetFilePath(path) \n",
    "# for i in gifPath:\n",
    "#   PreprocessGif(i)\n",
    "\n",
    "#arr=LoadGif(gifPath[3])\n",
    "#input, output=Divide(arr)\n",
    "#print(arr.shape ,input.shape, output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/82109/Desktop/datasets\\1.gif :  32  사용가능\n",
      "C:/Users/82109/Desktop/datasets\\2.gif :  31  사용가능\n",
      "C:/Users/82109/Desktop/datasets\\3.gif :  128  사용가능\n",
      "C:/Users/82109/Desktop/datasets\\smoke_loop_animation_by_alexredfish_d8uuq6h.gif :  45  사용가능\n",
      "C:/Users/82109/Desktop/datasets\\tomoe_kickslash_riser_animation_by_hybridmink_d9yq8fu.gif :  15  사용가능\n",
      "C:/Users/82109/Desktop/datasets\\train_by_kirokaze_d9q4jxs.gif :  29  사용가능\n",
      "C:/Users/82109/Desktop/datasets\\t_crouch_kicks_by_hybridmink_d952wjw.gif :  13  사용가능\n"
     ]
    }
   ],
   "source": [
    "#데이터셋 제너레이터 생성\n",
    "def DatasetGenerater(gifPath):\n",
    "  #gif파일을 반환\n",
    "  for i in gifPath:\n",
    "      x,y=Divide(LoadGif(i))  \n",
    "      yield (x,y)\n",
    "\n",
    "#사용불가능 파일 전처리\n",
    "gifPath=GetFilePath(path) \n",
    "for i in gifPath:\n",
    "  PreprocessGif(i)\n",
    "gifPath=GetFilePath(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset=tf.data.Dataset.from_generator(DatasetGenerater,\n",
    "                               args=[gifPath], output_types=(tf.float32,tf.float32),\n",
    "                               output_shapes = ((None, None,None,4),(None, None,None,4)))\n",
    "#(inputImages, outputImages)\n",
    "trainDataset=trainDataset.shuffle(10).batch(1).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unet 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seperableConv(filter, input):\n",
    "    depthwise=tf.keras.layers.Conv3D(input.shape[-1],3,padding=\"same\",groups=input.shape[-1])(input)\n",
    "    pointwise=tf.keras.layers.Conv3D(filter,1,padding=\"same\")(depthwise)\n",
    "    return pointwise\n",
    "    \n",
    "def block(filter,input):\n",
    "    #conv1=tf.keras.layers.Conv3D(filter,3,padding=\"same\")(input)\n",
    "    conv1=seperableConv(filter,input)\n",
    "    layerNorm1=tf.keras.layers.LayerNormalization()(conv1)\n",
    "    swishAct1=tf.keras.layers.Activation(\"swish\")(layerNorm1)\n",
    "\n",
    "    #conv2=tf.keras.layers.Conv3D(filter,3,padding=\"same\")(swishAct1)\n",
    "    conv2=seperableConv(filter,swishAct1)\n",
    "    layerNorm2=tf.keras.layers.LayerNormalization()(conv2)\n",
    "    swishAct2=tf.keras.layers.Activation(\"swish\")(layerNorm2)\n",
    "    \n",
    "    return swishAct2\n",
    "\n",
    "\n",
    "def unetModel(inputShape=(None, None, None, 4)):\n",
    "    inputImage=tf.keras.Input(shape=inputShape)\n",
    "    inputRate=tf.keras.Input(shape=(inputShape[0],inputShape[1],inputShape[2],1))\n",
    "    input=tf.keras.layers.Concatenate()([inputImage,inputRate])\n",
    "    \n",
    "    #인코딩\n",
    "    e1=block(32,input)\n",
    "    e1Pooling=tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(e1)\n",
    "    \n",
    "    e2=block(64,e1Pooling)\n",
    "    e2Pooling=tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(e2)\n",
    "    \n",
    "    e3=block(128,e2Pooling)\n",
    "    e3Pooling=tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(e3)\n",
    "    \n",
    "    e4=block(256,e3Pooling)\n",
    "    e4Pooling=tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(e4)\n",
    "    \n",
    "    e5=block(512,e4Pooling)\n",
    "    e5Pooling=tf.keras.layers.MaxPooling3D(pool_size=(1, 2, 2))(e5)\n",
    "    \n",
    "    #중간\n",
    "    bottleNeck=block(1024,e5Pooling)\n",
    "\n",
    "    d5UpSampling=tf.keras.layers.UpSampling3D(size=(1,2,2))(bottleNeck)\n",
    "    d5Transpose=seperableConv(512,d5UpSampling)\n",
    "    d5Concatenate=tf.keras.layers.Concatenate()([d5Transpose,e5])\n",
    "    d5=block(512,d5Concatenate)\n",
    "    \n",
    "    d4UpSampling=tf.keras.layers.UpSampling3D(size=(1,2,2))(d5)\n",
    "    d4Transpose=seperableConv(256,d4UpSampling)\n",
    "    d4Concatenate=tf.keras.layers.Concatenate()([d4Transpose,e4])\n",
    "    d4=block(256,d4Concatenate)\n",
    "    \n",
    "    d3UpSampling=tf.keras.layers.UpSampling3D(size=(1,2,2))(d4)\n",
    "    d3Transpose=seperableConv(128,d3UpSampling)\n",
    "    d3Concatenate=tf.keras.layers.Concatenate()([d3Transpose,e3])\n",
    "    d3=block(128,d3Concatenate)\n",
    "    \n",
    "    d2UpSampling=tf.keras.layers.UpSampling3D(size=(1,2,2))(d3)\n",
    "    d2Transpose=seperableConv(64,d2UpSampling)\n",
    "    d2Concatenate=tf.keras.layers.Concatenate()([d2Transpose,e2])\n",
    "    d2=block(64,d2Concatenate)\n",
    "    \n",
    "    d1UpSampling=tf.keras.layers.UpSampling3D(size=(1,2,2))(d2)\n",
    "    d1Transpose=seperableConv(32,d1UpSampling)\n",
    "    d1Concatenate=tf.keras.layers.Concatenate()([d1Transpose,e1])\n",
    "    d1=block(32,d1Concatenate)\n",
    "    \n",
    "    outputImage=seperableConv(4,d1)\n",
    "    \n",
    "    return tf.keras.Model([inputImage,inputRate],outputImage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디퓨전 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(tf.keras.Model):\n",
    "    def __init__(self,network:tf.keras.Model,batchSize=1):\n",
    "        super().__init__()\n",
    "        self.network= network\n",
    "        self.batchSize=batchSize\n",
    "        \n",
    "        \n",
    "    def train_step(self, images):\n",
    "        \n",
    "        noises = tf.random.normal(\n",
    "            shape=(self.batchSize, images.shape[1], images.shape[2], images.shape[3])\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # # sample uniform random diffusion times\n",
    "        # diffusion_times = tf.random.uniform(\n",
    "        #     shape=(1, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        # )\n",
    "        # noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        # # mix the images with noises accordingly\n",
    "        # noisy_images = signal_rates * images + noise_rates * noises\n",
    "\n",
    "        # with tf.GradientTape() as tape:\n",
    "        #     # train the network to separate noisy images to their components\n",
    "        #     pred_noises, pred_images = self.denoise(\n",
    "        #         noisy_images, noise_rates, signal_rates, training=True\n",
    "        #     )\n",
    "\n",
    "        #     noise_loss = self.loss(noises, pred_noises)  # used for training\n",
    "        #     image_loss = self.loss(images, pred_images)  # only used as metric\n",
    "\n",
    "        # gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
    "        # self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
    "\n",
    "        # self.noise_loss_tracker.update_state(noise_loss)\n",
    "        # self.image_loss_tracker.update_state(image_loss)\n",
    "\n",
    "        # # track the exponential moving averages of weights\n",
    "        # for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
    "        #     ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
    "\n",
    "        # # KID is not measured during the training phase for computational efficiency\n",
    "        # return {m.name: m.result() for m in self.metrics[:-1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=unetModel(inputShape=(None,None,None,4))\n",
    "model=DiffusionModel(network)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=savePath, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    )\n",
    "model.compile(\n",
    "    tf.keras.optimizers.experimental.AdamW(\n",
    "        learning_rate=1e-3, weight_decay=1e-4\n",
    "    ),\n",
    "    loss=tf.keras.losses.mean_squared_error,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 저장 복구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath=\"C:/Users/82109/Desktop/새 폴더/sup\"\n",
    "\n",
    "model=unetModel(inputShape=(None,None,None,4))\n",
    "model.save_weights(filepath=savePath)\n",
    "\n",
    "model=unetModel(inputShape=(None,None,None,4))\n",
    "model.load_weights(filepath=savePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "field",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
